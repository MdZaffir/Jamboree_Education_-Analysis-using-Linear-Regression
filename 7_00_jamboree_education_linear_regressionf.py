# -*- coding: utf-8 -*-
"""7.00 Jamboree Education - Linear Regressionf.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1X0uEk9QxUxXsZwclMwzezvMtOm8s-s_b
"""

# import the library
import numpy as np
import pandas as pd
import warnings

import numpy as np
import pandas as pd
path="/content/Jamboree_Admission.csv"
df=pd.read_csv(path)

display(df.head(1))

df.shape

"""Now, i am going to drop the irrelevant column and check if there are any null values in the dataset"""

df=df.drop('Serial No.',axis=1)

df.isnull().sum()

"""Now i am going to  see the distribution of the variables of graduate applicants"""

# importing the Library
import matplotlib.pyplot as plt
import seaborn as sns

column=df.columns

for col in column:
  df[col].plot(kind="hist",bins=10,label=col)
  plt.title("Distribution")
  plt.xlabel(col)
  plt.show()

sns.histplot(df['GRE Score'],kde=False)
plt.title(" Distribution of GRE Score ")
plt.show()

sns.histplot(df['TOEFL Score'],kde=False)
plt.title(" Distribution of TOEFL Score ")
plt.show()

sns.histplot(df['University Rating'],kde=False)
plt.title(" Distribution of University Rating ")
plt.show()

sns.histplot(df['SOP'],kde=False)
plt.title(" Distribution of SOP ")
plt.show()

sns.histplot(df['CGPA'],kde=False)
plt.title(" Distribution of CGPA ")
plt.show()

sns.histplot(df['Research'],kde=False)
plt.title(" Distribution of Research ")
plt.show()

"""From the above distribution  graph ,It is clear students with varied merit apply for the university.

"""

# Now i am going Understand the relation between diff erent factors responsible for graduateadmission

sns.regplot(x=df['GRE Score'],y=df['TOEFL Score'],color='g')
plt.title("Distribution of GRE and TOEFL")
plt.show()

"""From the above graph it shows that more of the student have similar score in both "GRE" and "TOEFL"."""

sns.regplot(x=df['GRE Score'],y=df['CGPA'],color='b')
plt.title("Distribution of GRE and CGPA")
plt.show()

sns.regplot(x=df['TOEFL Score'],y=df['CGPA'],color='r')
plt.title("Distribution of TOEFL Score and CGPA")
plt.show()

sns.scatterplot(x=df['TOEFL Score'],y=df['CGPA'], hue=df['Research'],color='r')
plt.title("Distribution of GRE and CGPA")
plt.show()

sns.scatterplot(x=df['CGPA'], y=df['SOP'])
plt.title("Distribution of CGPA and SOP")
plt.show()

"""CGPA and SOP are not that related because Statement of Purpose is related to academicperformance, but since people with good CGPA tend to be more hard working so they havegood things to say in their SOP which might explain the slight move towards higher CGPAas along with good SOPs"""

sns.scatterplot(x=df['TOEFL Score'],y=df['CGPA'], hue=df['SOP'],color='r')
plt.title("Distribution of TOEFL Score and CGPA")
plt.show()

"""As from the above graph , student having Good TOEFL Score,CGPA and SOP  are related to each other  because Statement of Purpose is related to academicperformance, but since people with good CGPA tend to be more hard working so they havegood things to say in their SOP which might explain the slight move towards higher CGPAas along with good SOPs"""

sns.scatterplot(x=df['TOEFL Score'],y=df['CGPA'], hue=df['LOR '],color='r')
plt.title("Distribution of TOEFL Score and LOR ")
plt.show()

cor=df.corr()
sns.heatmap(df.corr(),linewidths=0.5,annot=True)

"""From the above heat map it is abosorve that TOEFL Score,GRE  Score and CGPA are highly correlated."""

sns.pairplot(df)

"""Now i am going split the data set into two parts , train data set and test data to perform linear Regression Analysis. Here data is very less so we split the data into Train and test not to validat"""

from sklearn.model_selection import train_test_split
y=df['Chance of Admit ']
x=df.drop('Chance of Admit ',axis=1)
xtrain,xtest,ytrain,ytest=train_test_split(x,y,test_size=0.2,random_state=42)
xtrain.shape,ytrain.shape,xtest.shape,ytest.shape

"""Now i am going to make a model for Linear regression,Lasso Regression and Ridge Regression and find the accuracy values and MSE(Mean Squared error)"""

from sklearn.linear_model import LinearRegression,Lasso,Ridge
from sklearn.metrics import accuracy_score,mean_squared_error
models=[
    ["modelLinear :",LinearRegression()],
    ["modelLasso :",Lasso()],
    ["modelRidge :",Ridge()]]
print("Results without removing features with multicollinearity ...")
for name,model in models:
  model.fit(xtrain,ytrain)
  predictions=model.predict(xtest)
  print(name,np.sqrt(mean_squared_error(ytest,predictions)))

model.coef_



model.intercept_

yp=model.predict(xtest)

error=yp-ytest
sns.histplot(error)
plt.title("Distribution of Error ")

"""Linear Regression using Statsmodel library
Adjusted. R-squared refl ects the fi t of the model. R-squared values range from 0 to 1,where a higher value generally indicates a better fi t, assuming certain conditions aremet.
const coeffi cient is your Y-intercept. It means that if both the Interest_Rate andUnemployment_Rate coeffi cients are zero, then the expected output (i.e., the Y) wouldbe equal to the const coeffi cient.
Interest_Rate coeffi cient represents the change in the output Y due to a change of oneunit in the interest rate (everything else held constant)
Unemployment_Rate coeffi cient represents the change in the output Y due to a changeof one unit in the unemployment rate (everything else held constant)
std err refl ects the level of accuracy of the coeffi cients. The lower it is, the higher is thelevel of accuracy
P >|t| is your p-value. A p-value of less than 0.05 is considered to be statisticallysignifi cant
Confi dence Interval represents the range in which our coeffi cients are likely to fall (witha likelihood of 95%)
"""

import statsmodels.api as sm
xtrain=sm.add_constant(xtrain)
model=sm.OLS(ytrain,xtrain).fit()
print(model.summary())

""" As pvalues of SOP is greater  we are going to drop it"""

xtrain1=xtrain.drop('SOP',axis=1)

model=sm.OLS(ytrain,xtrain1).fit()
print(model.summary())

"""VIF(Variance Infl ation Factor)
â€œ VIF score of an independent variable represents how well the variable is explained byother independent variables.
So, the closer the R^2 value to 1, the higher the value of VIF and the higher themulticollinearity with the particular independent variable.
"""

import numpy as np
import pandas as pd
path="/content/Jamboree_Admission.csv"
df=pd.read_csv(path)

from sklearn.preprocessing import StandardScaler
df=pd.DataFrame(StandardScaler().fit_transform(df),columns=df.columns)
np.round(df,4).head()

from sklearn.model_selection import train_test_split
y=df['Chance of Admit ']
x=df.drop('Chance of Admit ',axis=1)
xtrain,xtest,ytrain,ytest=train_test_split(x,y,test_size=0.2,random_state=42)
xtrain.shape,ytrain.shape,xtest.shape,ytest.shape

from statsmodels.stats.outliers_influence import variance_inflation_factor
vif=pd.DataFrame()
x_t=pd.DataFrame(xtrain,columns=xtrain.columns)
vif['feature']=xtest.columns
vif['VIF']=[variance_inflation_factor(xtrain.values,i) for i in range(xtrain.shape[1])]
vif.sort_values(by='VIF',ascending=False)

"""Here VIF score all are less than 5 , there is no multicolinearity."""

from sklearn.linear_model import LinearRegression,Lasso,Ridge
from sklearn.metrics import accuracy_score,mean_squared_error,mean_absolute_error,r2_score
models=[
    ["modelLinear :",LinearRegression()],
    ["modelLasso :",Lasso()],
    ["modelRidge :",Ridge()]]
print("Results without removing features with multicollinearity ...")
for name,model in models:
  model.fit(xtrain,ytrain)
  predictions=model.predict(xtest)
  print("Mean Square Error of Model:",name,np.sqrt(mean_squared_error(ytest,predictions)))
  print("Mean Absolute Error of Model:",name,np.sqrt(mean_absolute_error(ytest,predictions)))

"""Mean of Residuals"""

residual=ytest-predictions
Mean_residule=np.mean(residual)
print("Mean_Residule:",Mean_residule)

"""Test for Homoscedasticity"""

from scipy import stats
res=stats.shapiro(residual)
res.statistic
sns.scatterplot(x=predictions,y=residual)
plt.xlabel("Predicted Values By Model")
plt.ylabel("Residuals Value")
plt.title("Graph of Predicted V/s Residuals")
plt.show()

"""Normality of residuals"""

sns.histplot(residual,kde=True)
plt.title("Normality of Residuals(Errors)")

"""# Plotting y_test and y_pred to understand the spread."""

sns.scatterplot(x=ytest,y=predictions)
plt.title("Ytest V/S Y Predicted")
plt.xlabel(" Y test values")
plt.ylabel("Y Predicted Values ")

